%**********************************************************************

%---------------------------------------------------
% NOTE:
% An English version of the abstract is always required 
% (even for German BA/MAs).
%---------------------------------------------------

% right side/flush
\chapterend

\begin{titlepage}

\begin{otherlanguage}{english} 

\begin{abstract} % Abstract
\label{abstract_english}
This thesis explores the improvement of web-crawlers, pivotal for systematic information retrieval from the internet. Given the vast digital landscape, encompassing approximately 3 billion websites, this study focuses on enhancing the efficiency and scalability of web-crawlers. The primary objective is to investigate methods for increasing the scalability of web-crawlers, enabling the efficient download and processing of a vast amount of web pages, while ensuring efficient resource use and operation distribution. In response to this challenge, a prototype of a web-crawler, which emphasizes distributed operations and resource efficiency, was developed. The research methodology included a comparative analysis of existing web-crawling technologies, leading to the design, implementation, and evaluation of an innovative prototype. This prototype leverages a microservice-based architecture and Ahead-Of-Time (AOT) compilation, as opposed to traditional Just-In-Time (JIT) compilation, to address the limitations of current web-crawling methods. The findings demonstrated significant improvements in the scalability and efficiency of web-crawling operations. By optimizing task distribution and resource allocation, the prototype showed enhanced capability in handling extensive workloads more effectively than traditional methods. These improvements support the rapid and efficient analysis of large datasets, advancing data management and retrieval in the growing digital environment. This research contributes to the field of digital information retrieval by offering a scalable solution to web-crawling challenges. It establishes a foundation for further studies to enhance web-crawler capabilities.

\end{abstract}

\end{otherlanguage}


\end{titlepage}


%---------------------------------------------------
% NOTE:
% A German version of the abstract "Zusammenfassung"
% is always required.
%---------------------------------------------------

\begin{titlepage}

\begin{otherlanguage}{german}

\begin{abstract}  % Zusammenfassung
\label{abstract_german}

Diese Arbeit beschäftigt sich mit der Verbesserung von Web-Crawlern, die für die systematische Informationsbeschaffung im Internet von zentraler Bedeutung sind. Angesichts der großen digitalen Landschaft, die schätzungsweise 3 Milliarden Webseiten umfasst, konzentriert sich diese Studie auf die Verbesserung der Effizienz und Skalierbarkeit von Web-Crawlern. Das Hauptziel ist die Untersuchung von Methoden zur Verbesserung der Skalierbarkeit von Web-Crawlern, die das effiziente Herunterladen und Verarbeiten einer großen Menge von Webseiten ermöglichen und gleichzeitig eine effiziente Ressourcennutzung und Betriebsverteilung gewährleisten. Als Antwort auf diese Herausforderung wurde ein Prototyp eines Web-Crawlers entwickelt, der den Schwerpunkt auf verteilte Operationen und Ressourceneffizienz legt. Die Forschungsmethodik umfasste eine vergleichende Analyse bestehender Web-Crawling-Technologien, die zur Konzeption, Implementierung und Bewertung eines innovativen Prototyps führte. Dieser Prototyp nutzt eine Microservice-basierte Architektur und AOT-Kompilierung (Ahead-Of-Time) im Gegensatz zur traditionellen Just-In-Time-Kompilierung (JIT), um die Grenzen der derzeitigen Web-Crawling-Methoden zu überwinden. Die Ergebnisse zeigen, dass die Skalierbarkeit und Effizienz der Web-Crawling-Operationen erheblich verbessert werden konnten. Durch die Optimierung der Aufgabenverteilung und Ressourcenzuweisung konnte der Prototyp umfangreiche Arbeitslasten effektiver bewältigen als herkömmliche Methoden. Diese Verbesserungen unterstützen die schnelle und effiziente Analyse großer Datenmengen und fördern die Datenverwaltung und -abfrage in der wachsenden digitalen Umgebung. Diese Forschungsarbeit leistet einen Beitrag zum Bereich der digitalen Informationsbeschaffung, indem sie eine skalierbare Lösung für die Herausforderungen des Web-Crawling bietet. Sie schafft eine Grundlage für weitere Studien zur Verbesserung der Fähigkeiten von Web-Crawlern.

\end{abstract}

\end{otherlanguage}

\end{titlepage}

%**********************************************************************
